
\input{preambuleReport}



\title{New stochastic sketching methods for Big Data Ridge Regression}
\author{Cheikh Saliou Tour\'e \\ \\
Student at ENS Cachan\\\\
Tutor : Robert Gower \\ \\
Inria Paris (Sierra department)\\\\ }




\date{July, 2017}


\begin{document}

\renewcommand\bibname{References}
%\renewcommand\contentsname{Table of contents}
\maketitle


\begin{abstract}

//

\end{abstract}
\tableofcontents
\newpage

\chapter{Randomized Newton Method}

%\ecag{11}{green}{19}{
%\begin{definition}
%
%
%
%\end{definition} 
%}


\section{Algorithm}

\section{Convergence rate (draft)}


%For $p_{i} = \frac{Tr(I_{C_{i}} A I_{C_{i}}^{T})}{\norm{A^{\frac{1}{2} S }^{2}_{F}}$ for $i = 1,\dots,r$,\\

% [reference here] shows that $\rho \leq 1 - \frac{\lambda_{min}(S^{T}A\bold{S})}{\norm{A^{\frac 12} \bold{S} }^{2}_{F}}$.\\

$Z = A I_{C}^{T} (I_{C} A I_{C}^{T})^{-1} I_{C} A$\\

$\rho = 1 - \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}  )$.

$A^{-\frac12}E[Z]A^{-\frac12} = \dsp\sum\limits_{i} p_{i} A^{\frac 12} I_{C_{i}}^{T} (I_{C_{i}}  A  I_{C_{i}}^{T})^{-1} I_{C_{i}} A^{\frac 12}$ \\

for any $i \in \acco{1,\dots,n}$, $A^{\frac 12} I_{C_{i}}^{T} (I_{C_{i}}  A  I_{C_{i}}^{T})^{-1} I_{C_{i}} A^{\frac 12}$ is a projection matrix and then its eigenvalues are a nonempty subset of $\acco{0,1}$.\\

Since $\lambda_{max}$ is convex, we obtain that :\\

$0 \leq \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}) \leq  \lambda_{max}(A^{-\frac12}E[Z]A^{-\frac12}) \leq \dsp\sum\limits_{i} p_{i} \lambda_{max}(A^{\frac 12} I_{C_{i}}^{T} (I_{C_{i}}  A  I_{C_{i}}^{T})^{-1} I_{C_{i}} A^{\frac 12}) \leq 1$.\\

$\bold{C} = (I_{C_{1}}^{T},\dots,I_{C_{r}}^{T})$.\\

$A^{-\frac12}E[Z]A^{-\frac12} = (A^{\frac 12} \bold{C} D)(D \bold{C}^{T} A^{\frac 12})$ where $D = \sqrt{p} \,\text{diag}( (S_{1}AS_{1}^{T})^{-\frac 12},\dots, (S_{r}AS_{r}^{T})^{-\frac 12})$ wherein $p = \dsp\frac{1}{\pare{\substack{ n \\ s }}}$ is the uniform probability of choosing $s$ rows uniformly on $\acco{1,\dots,n}$, knowing that $s$ is the sketchsize. \\

\ecag{11}{red}{19}{
\begin{proposition} [Unifom sketching]
$\\\\$
$\dsp \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} ) \geq \frac{1}{\pare{\substack{n \\ s}}} \frac{\lambda_{min}(A)}{\lambda_{max}(A)} $

\end{proposition}
}


And then :\\
$\lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} ) \geq \lambda_{min}(\bold{C}^{T}A\bold{C} ) \lambda_{min}(D^{2})$

[ref Rob thesis] \\

$\dsp \lambda_{min}(D^{2}) = p \min_{i} \frac{1}{\lambda_{max}(I_{C_{i}} A I_{C_{i}}^{T} ) } = p \min_{i} \frac{1}{\lambda_{max}(I_{C_{i}}^{T} I_{C_{i}}) \lambda_{max}(A)}  \geq \frac{p}{\lambda_{max}(A)} $, 
since for any $i\in \acco{1,\dots,n}$, for any $x$ in $\R^{n}$ 
$\scal{I_{C_{i}}^{T} I_{C_{i}} x }{x} =$
$ \norm{ I_{C_{i}}x }^{2} \leq \norm{x}^{2}$
 and then $\lambda_{max}( I_{C_{i}}^{T} I_{C_{i}}  ) \leq 1$.\\
 
 Therefore, 
$\lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} ) \geq p \frac{\lambda_{min}(\bold{C}^{T}A\bold{C} )}{\lambda_{max}(A)}  = p \frac{ \lambda_{min}(A) \lambda_{min}(\bold{C} \bold{C}^{T} )}{\lambda_{max}(A)}  \geq p \frac{\lambda_{min}(A)}{\lambda_{max}(A)}$


 
 
 

\chapter{Hadamard Sketches}

%\ecag{11}{green}{19}{
%\begin{definition}
%
%
%
%\end{definition} 
%}


\section{Algorithm}


\section{Convergence rate (draft)}

$Z = A S^{T} (S A S^{T})^{-1} S A$\\


$\rho = 1 - \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}  )$

$S_{i} = I_{C_{i}} H.$\\

%$\bold{S} = [I_{C_{1}},I_{C_{2}},\dots,I_{C_{r}}] H$.\\
%Denote $I_{C} = [I_{C_{1}},I_{C_{2}},\dots,I_{C_{r}}].$\\
$\tilde{A} = \frac{1}{n} H A H^{T} = \frac{H}{\sqrt{n}} A \frac{H^{T}}{\sqrt{n}}.$\\

%$E[Z] = (A \bold{S}^{T} D) (D \bold{S} A) = (A H^{T} I_{C}^{T} D) (D I_{C} H A) = $

$A^{-\frac12}E[Z]A^{-\frac12} = E[A^{\frac 12} S^{T} (S A S^{T})^{-1} S A^{\frac 12} ] = \dsp\sum\limits_{i} p_{i} A^{\frac 12} H^{T} I_{C_{i}}^{T} (I_{C_{i}} H A H^{T} I_{C_{i}}^{T})^{-1} I_{C_{i}} H A^{\frac 12} \\
= A^{\frac 12} H^{T} \frac{1}{n} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] H A^{\frac 12}  = H^{-1} \tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12} (\frac{H^{T}}{n})^{-1} =
H^{-1} \tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12} H$.\\
Hence :\\

$\rho = 1 - \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}) = 1 - \lambda_{min}(\tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12} ) $



We recognize the convergence rate in the Randomized Newton Method and then, denoting by $\rho_{Newton}(M)$ the convergence rate of the Newton method associated with the definite positive matrix $M$, we obtain that :\\
$\rho = 1 - \lambda_{min}( \tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12}) = 1 -  (1 - \rho_{Newton}(\tilde{A} ) ) =  \rho_{Newton}(\tilde{A} ) $


\chapter{Count-min Sketches}

%\ecag{11}{green}{19}{
%\begin{definition}
%
%
%
%\end{definition} 
%}

\section{Algorithm}


\section{Convergence rate}

$Z = A S^{T} (S A S^{T})^{-1} S A$\\


\chapter{Conclusion}

\appendix
\begin{thebibliography}{1}

\bibitem{}
{\sc Robert Gower and Peter Richtarik}, {\em Randomized iterative methods for linear systems}, SIAM, 
  (2015).



\end{thebibliography}

\end{document}



