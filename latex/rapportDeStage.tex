
\input{preambuleReport}

\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\newcommand{\rob}[1]{\todo[inline]{\textbf{Robert: }#1}}

\title{New stochastic sketching methods for Big Data Ridge Regression}
\author{Cheikh Saliou Tour\'e \\ \\
Student at ENS Cachan\\\\
Tutor : Robert Gower \\ \\
Inria Paris (Sierra department)\\\\ }




\date{July, 2017}


\begin{document}

\renewcommand\bibname{References}
%\renewcommand\contentsname{Table of contents}
\maketitle


\begin{abstract}

//

\end{abstract}
\tableofcontents
\newpage

\chapter{Randomized Newton Method} \label{newton}

%\ecag{11}{green}{19}{
%\begin{definition}
%
%
%
%\end{definition} 
%}


\section{Algorithm}

\section{Convergence rate (draft)}

\subsection{General case}
$A$ is a $n \times n$ positive definite matrix representing our problem.\\ 
For $C$ any subset of $\acco{1,\dots,n}$ of length $s$, we denote by $I_{C}$ the $s\times n$ matrix which rows are $\acco{e_{i}^{T}}_{i\in C}$ up to a permutation, where $\acco{e_{i}}_{i=1,\dots,n}$ is a canonical basis of $\R^{n}.$\\ 
 
Throughout the computations, we denote by $Z = A I_{C}^{T} (I_{C} A I_{C}^{T})^{-1} I_{C} A$. That is a quantity that intervenes in the computation of the convergence rate.\\

The convergence rate is defined by $\rho = 1 - \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}  )$.\\

By defiition, $A^{-\frac12}E[Z]A^{-\frac12} = \dsp\sum\limits_{i} p_{i} A^{\frac 12} I_{C_{i}}^{T} (I_{C_{i}}  A  I_{C_{i}}^{T})^{-1} I_{C_{i}} A^{\frac 12}$ \\

for any $i \in \acco{1,\dots,n}$, $A^{\frac 12} I_{C_{i}}^{T} (I_{C_{i}}  A  I_{C_{i}}^{T})^{-1} I_{C_{i}} A^{\frac 12}$ is a projection matrix and then its eigenvalues are a nonempty subset of $\acco{0,1}$.\\

Since $\lambda_{max}$ is convex, we obtain that :\\

$0 \leq \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}) \leq  \lambda_{max}(A^{-\frac12}E[Z]A^{-\frac12}) \leq \dsp\sum\limits_{i} p_{i} \lambda_{max}(A^{\frac 12} I_{C_{i}}^{T} (I_{C_{i}}  A  I_{C_{i}}^{T})^{-1} I_{C_{i}} A^{\frac 12}) \leq 1$.\\

Denote by $\bold{C} = (I_{C_{1}}^{T},\dots,I_{C_{r}}^{T})$ which is of size $ n \times r s$.\\

$A^{-\frac12}E[Z]A^{-\frac12} = (A^{\frac 12} \bold{C} D)(D \bold{C}^{T} A^{\frac 12})$ where \\$D =  \,\text{diag}(\sqrt{p_{1}} (I_{C_{1}}A I_{C_{1}}^{T})^{-\frac 12},\dots, \sqrt{p_{r}}(I_{C_{r}} A I_{C_{r}}^{T})^{-\frac 12}) \in \M_{r s}(\R)$
 
 
 \ecag{11}{red}{19}{
\begin{proposition} [Unifom sketching] \label{unif}
$\\\\$
$\dsp \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} )  \geq  \dsp \pare{\substack{n-1 \\ s-1}}\frac{\lambda_{min}(A)}{\lambda_{max}(A)}\min_{i} p_{i} $

\end{proposition}
}
\pr 

$\lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} ) \geq \lambda_{min}(\bold{C}^{T}A \bold{C} ) \lambda_{min}(D^{2})$

$\dsp \lambda_{min}(D^{2}) =  \min_{i}  \frac{p_{i}}{\lambda_{max}(I_{C_{i}} A I_{C_{i}}^{T} ) } \geq  \min_{i}\frac{p_{i} }{\lambda_{max}(I_{C_{i}}^{T} I_{C_{i}}) \lambda_{max}(A)}  \geq \min_{i}\frac{ p_{i} }{\lambda_{max}(A)} $, 
since for any $i\in \acco{1,\dots,n}$, for any $x$ in $\R^{n}$ 
$\scal{I_{C_{i}}^{T} I_{C_{i}} x }{x} =$
$ \norm{ I_{C_{i}}x }^{2} \leq \norm{x}^{2}$
 and then $\lambda_{max}( I_{C_{i}}^{T} I_{C_{i}}  ) \leq 1$.\\
 
 Therefore, 
$\dsp \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} ) \geq  \min_{i} p_{i} \frac{\lambda_{min}(\bold{C}^{T}A \bold{C} )}{\lambda_{max}(A)}  =  \min_{i} p_{i} \frac{ \lambda_{min}(A) \lambda_{min}(\bold{C} \bold{C}^{T} )}{\lambda_{max}(A)}.$\\

$\bold{C} \bold{C}^{T}= \dsp\sum\limits_{i} I_{C_{i}}^{T} I_{C_{i}} = \pare{\substack{n-1 \\ s-1}} I_{n} $ and then we obtain that :\\ 


$\dsp \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} )  \geq  \dsp \pare{\substack{n-1 \\ s-1}}\frac{\lambda_{min}(A)}{\lambda_{max}(A)}\min_{i} p_{i} $


\subsection{Uniform case}

For any $i$, $p_{i} = \dsp\frac{1}{\pare{\substack{ n \\ s }}}$ is the uniform probability of choosing $s$ rows uniformly on $\acco{1,\dots,n}$, knowing that $s$ is the sketch size. That leads towards that corollary of \textbf{Proposition} \ref{unif} :\\


\ecag{11}{green}{19}{
\begin{corollary} [Unifom sketching]
$\\\\$
$\dsp \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} ) \geq \frac{s}{n} \frac{\lambda_{min}(A)}{\lambda_{max}(A)} $

\end{corollary}
}
 \rob{This is already pretty interesting! It shows an improvement for using bigger bachsize! We should try to push this further, for instance, when $s =n$ we know the method converges in one step. It would be great if we have a convergence rate that shows this phenomena. In other words, when $s =n$ we have $\lambda_{\min}(A^{-1/2}E[Z]A^{-1/2}) =1$ ! Also, please have a look at the paper ``paving\_kaczmarz.pdf'' which I've just added to our repo.}
 
 \subsection{A convenient probability}
 
 Suppose here that $\dsp p_{i} = \frac{Tr( I_{C_{i}} A I_{C_{i}}^{T} ) }{\norm{A^{\frac12}\bold{C}}^{2}_{F} }$, for any $i = 1,\dots,r.$\\

 

\chapter{Randomized orthonormal systems}

This type of randomized system is well-suited for big data regression, thanks to the efficiency of matrix multiplication used in this method.\\
When the dimension of our matrix $A$ is $n$, we denote by $H_{n}$ the Hadamard matrix (well defined if the dimension of the problem $n$ is a power of $2$) defined recursively as :\\

$H_{p} = ..$ and $H_{1} = 1.$\\

The Hadamard sketch consists of choosing a sketch matrix $S \in \M_{s,n}$ where $s$ is called the sketch size of the problem, as follows :\\ 
we sample $s$ $i.i.d.$ rows of the form $s^{T} = e_{j}^{T}H_{n} D $ with probability $\frac 1n$ for $j = 1,\dots,n$,where $(e_{j})_{j}$ forms a canonical base of $\R^{n}$, and $D = diag(\nu)$ is a diagonal matrix of $i.i.d.$ Rademacher variables $\nu \in \acco{-1,1}^{n}$.  

%\ecag{11}{green}{19}{
%\begin{definition}
%
%
%
%\end{definition} 
%}


\section{Algorithm}


\section{Convergence rate (draft)}



Now we denote by $Z = A S^{T} (S A S^{T})^{-1} S A$, where $S$ is our Hadamard random matrix.\\


The convergence rate is then $\rho = 1 - \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}  )$

Notice that $S_{i} = I_{C_{i}} H D.$ where $(C_{i})_{i}$ are uniform random subsets of $\acco{1,\dots,n}$ of size $s$, as defined in the $Randomized \,\,Newton$ section \ref{newton}.\\

Let's condition on the Rademacher diagonal matrix $D$.\\

Define by $\tilde{A}_{D} = \frac{H} {\sqrt{n}} D A D \frac{H^{T}}{\sqrt{n}}$. We obtain that :

\baStar
A^{-\frac12}E[Z|D]A^{-\frac12} &=& E[A^{\frac 12} S^{T} (S A S^{T})^{-1} S A^{\frac 12}|D ] \\
&=& \dsp\sum\limits_{i} p_{i} A^{\frac 12} D H^{T} I_{C_{i}}^{T} (I_{C_{i}} H D A D H^{T} I_{C_{i}}^{T})^{-1} I_{C_{i}} H D A^{\frac 12} \\
&=& A^{\frac 12}D H^{T} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] HD A^{\frac 12} \\
 &=& n D H^{-1} \tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12} n (H^{T})^{-1} D\\
  &=& D H^{T} \tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12} H D.
  \eaStar
  
  
  
  %%%%%%%%%%%
  \emph{(following to be changed)}

Hence :\\

$\rho = 1 - \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12}) = 1 - \lambda_{min}(\tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12} ) $


We recognize the convergence rate in the Randomized Newton Method and then, denoting by $\rho_{Newton}(M)$ the convergence rate of the Newton method associated with the definite positive matrix $M$, we obtain that :\\
$\rho = 1 - \lambda_{min}( \tilde{A}^{\frac 12} E[ I_{C}^{T} (I_{C} \tilde{A} I_{C}^{T})^{-1} I_{C} ] \tilde{A}^{\frac 12}) = 1 -  (1 - \rho_{Newton}(\tilde{A} ) ) =  \rho_{Newton}(\tilde{A} ) = \rho_{Newton}(A) $, since $A$ and $\tilde{A}$ have the same eigenvalues.\\

\ecag{11}{red}{19}{
\begin{proposition} [Unifom sketching]
$\\\\$
$\dsp \lambda_{min}(A^{-\frac12}E[Z]A^{-\frac12} ) \geq \frac{s}{n} \frac{\lambda_{min}(A)}{\lambda_{max}(A)} $

\end{proposition}
}



\chapter{Count-min Sketches}

%\ecag{11}{green}{19}{
%\begin{definition}
%
%
%
%\end{definition} 
%}

\section{Algorithm}


\section{Convergence rate}

$S$ is constructed as follows :\\
For every $i\in\acco{1,\dots,n}$, $l$ is chosen uniformly on $\acco{1,\dots,n}$ and $\epsilon$ uniformly on $\acco{-1,1}$, then $S$ is updated in his $l^{th}$ row as :\\
$S(l, :) := S(l,:) + \epsilon \, e_{i}^{T}$, where $e_{i}^{T}$ is the $i^{th}$ coloumn of the identity matrix.\\\\



$\bold{C} = (S_{1}^{T},\dots,S_{r}^{T})$ and $\lambda_{max}(S_{i}^{T}S_{i}) = \lambda_{max}(S_{i}S_{i}^{T}).$\\
$S_{i} S_{i}^{T} = \dsp\sum_{i,k} f_{\pi(j)}e_{j}^{T} e_{k}f_{\pi(k)}^{T}$.\\


\chapter{Conclusion}

\appendix
\begin{thebibliography}{1}

\bibitem{}
{\sc Robert Gower and Peter Richtarik}, {\em Randomized iterative methods for linear systems}, SIAM, 
  (2015).



\end{thebibliography}

\end{document}



